import os
import uuid
import shutil
from pathlib import Path
from typing import Optional
from fastapi import UploadFile
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters.character import RecursiveCharacterTextSplitter  
from langchain_community.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from transformers import AutoTokenizer, pipeline
from langchain.chains import RetrievalQA
from langchain import hub
from langchain.prompts import PromptTemplate

class RagPipeline:
    """Pipeline for Retrieval Augmented Generation."""
    def __init__(self, upload_dir: Path = Path(r"uploads/")):
        """Intialize with upload directory.
        
        Args:
            upload_dir (Path): Directory to save uploaded files. Default "uploads/"
        """
        self.upload_dir = upload_dir
        self.upload_dir.mkdir(parents=True, exist_ok=True)

    def upload_file(self, file: UploadFile):
        """Accept and save the file, Updates the file path.
        
        Args:
            file (UploadFile): Uploaded file from FastAPI.

        """
        try:
            # Save the file
            unique_filename = f"{uuid.uuid4().hex}_{file.filename}"
            file_path = self.upload_dir / unique_filename
            with file_path.open("wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            # file saved
            self.file_path = file_path
        except Exception as e:
            # Code to run if exception occurs
            print(e)
            return 
        
    def create_vectordb(self):
        """Checks and Loads the document from the file path and creates the vectordb.

        Returns:
            Optional[file]: Returns the vectordb for the RAG. None if fails
        """
        try:
            # Checking the document
            str_file_path = str(self.file_path)
            if str_file_path.endswith(".pdf"):
                loader = PyPDFLoader(self.file_path)
                
            elif str_file_path.endswith(".txt"):
                loader = TextLoader(self.file_path)
                
            # Loading the document
            documents = loader.load()
            # Flushing the saved document
            # if os.path.exists(self.file_path):
            #     os.remove(self.file_path)
                
            # Split document for storing in vector database
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            docs = text_splitter.split_documents(documents)

            # Path to the pre-trained model 
            modelPath = "sentence-transformers/all-MiniLM-l6-v2"

            # Model configuration options, specifying to use the CPU for computations
            model_kwargs = {'device':'cpu'}

            # Encoding options, specifically setting 'normalize_embeddings' to False
            encode_kwargs = {'normalize_embeddings': False}

            # HuggingFaceEmbeddings with the specified parameters
            embeddings = HuggingFaceEmbeddings(
                model_name=modelPath,     # pre-trained model's path
                model_kwargs=model_kwargs, # Model configuration options
                encode_kwargs=encode_kwargs # Encoding options
            )
            
            # Create and store documents in vector database
            vector_db=FAISS.from_documents(docs, embeddings)
            
            return vector_db
        except Exception as e:
            # Code to run if exception occurs
            print(e)
            return None 
        
    def model_pipeline(self):
        """Create model pipeline for generation
        
        Returns:
            pipeline: question answer huggingface pipeline.
        """
        try:
            
            hf = HuggingFacePipeline.from_model_id(
                model_id="openai-community/gpt2",
                task="text-generation",
                pipeline_kwargs={"temperature": 0.75, "max_new_tokens": 300}
            )

            llm = hf

            return llm
        except Exception as e:
            # Code to run if Exception occurs
            print(e)
            return 

    def rag_generator(self,query, vector_db, llm):
        """Generates responses based on retrieved information from vector database and query.
        Args:
            query: User's query to the system.
            vector_db: vector database with loaded documents.
            llm: Hugginface LLM pipeline for RAG
        Returns:
            response: The response generated by the rag pipeline.
        """
        try:
            # Retrieving relevant documents from vector database upto 4 documents
            retriever = vector_db.as_retriever(search_kwargs={"k": 4})

            # Prompt Template for RAG
            prompt_template = """Use the following pieces of context to answer the question at the end. Please follow the following rules:
            1. If you don't know the answer, don't try to make up an answer. Just say "I can't find the final answer but you may want to check the following links".
            2. If you find the answer, write the answer in a concise way with five sentences maximum.

            {context}

            Question: {question}

            Helpful Answer:
            """

            PROMPT = PromptTemplate(
            template=prompt_template, input_variables=["context", "question"]
            )

            # Creating retrieval chain
            retrievalQA = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=False,
                chain_type_kwargs={"prompt": PROMPT}
            )
            # Invoking the chain with query
            result = retrievalQA.invoke({"query": query})
            # print(result['result'])
            return result
        except Exception as e:
            # Code to run if Exception occurs
            print(e)
            return 


if __name__ == "__main__":
    ragpipe = RagPipeline()
    ragpipe.file_path = Path('RAG-System/uploads/FIL_Stock Market.pdf')

    vector_db = ragpipe.create_vectordb()
    if vector_db is None:
        print("Vector DB creation failed.")
    else:
        print("Vector DB created.")

    llm = ragpipe.model_pipeline()
    if llm is None:
        print("Model pipeline creation failed.")
    else:
        print("Model pipeline created.")

    queries = ["what is stock Market", 'What is Secondary Market', 'explain Equity Investment']
    for query in queries:
        result = ragpipe.rag_generator(query, vector_db, llm)
        if result is None:
            print("RAG generator failed.")
        else:
            print("Final Result:\n", result['result'])


                    
        
    